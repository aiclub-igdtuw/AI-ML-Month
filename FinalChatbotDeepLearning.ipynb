{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalChatbotDeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyaaa-1/AI-ML-Month/blob/main/FinalChatbotDeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "HJfEOuM4HTFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3d3022-ae0e-4ef0-9267-35bcc8509a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import tflearn\n",
        "import string\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer =  LancasterStemmer()\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tflearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c2eZlhYbHgC",
        "outputId": "cac1a9b2-33bd-4d7c-d9f4-6d3acea1b3d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=ffac64085b6fdc01bd8eac9bbc48ce19f07ae26333c63b5b2295ce0096cc82a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/14/2e/1d8e28cc47a5a931a2fb82438c9e37ef9246cc6a3774520271\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGwYY_4PZcOw",
        "outputId": "f017b378-d8ab-417b-8a17-768d94a230fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"intent (1).json\") as file:\n",
        "  data=json.load(file)\n",
        "try:\n",
        "  with open(\"data.pickle\",\"rb\") as f:\n",
        "    words,labels,training,output=pickle.load(f)\n",
        "except:\n",
        "  words=[]\n",
        "  labels=[]\n",
        "  docs_x=[]\n",
        "  docs_y=[]\n",
        "\n",
        "  for intent in data[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "      wrds=nltk.word_tokenize(pattern)\n",
        "      words.extend(wrds)\n",
        "      docs_x.append(wrds)\n",
        "      docs_y.append(intent[\"tag\"])\n",
        "\n",
        "    if intent[\"tag\"] not in labels:\n",
        "      labels.append(intent[\"tag\"])\n",
        "\n",
        "  words=[stemmer.stem(w.lower()) for w in words if w not in \"?\"]\n",
        "  words=sorted(list(set(words)))\n",
        "  labels=sorted(labels)\n",
        "  training=[]\n",
        "  output=[]\n",
        "  out_empty=[0 for _ in range(len(labels))]\n",
        "\n",
        "  for x,doc in enumerate(docs_x):\n",
        "    bag=[]\n",
        "\n",
        "    wrds=[stemmer.stem(w.lower()) for w in doc]\n",
        "    for w in words:\n",
        "      if w in wrds:\n",
        "        bag.append(1)\n",
        "      else:\n",
        "        bag.append(0)\n",
        "\n",
        "    output_row=out_empty[:]\n",
        "    output_row[labels.index(docs_y[x])]=1\n",
        "\n",
        "    training.append(bag)\n",
        "    output.append(output_row)\n",
        "\n",
        "  training=np.array(training)\n",
        "  output=np.array(output)\n",
        "\n",
        "  with open(\"data.picle\",\"wb\") as f:\n",
        "    pickle.dump((words,labels,training,output),f)\n",
        "\n",
        "ops.reset_default_graph()\n",
        "\n",
        "net=tflearn.input_data(shape=[None,len(training[0])])\n",
        "net=tflearn.fully_connected(net,8)\n",
        "net=tflearn.fully_connected(net,8)\n",
        "net=tflearn.fully_connected(net,len(output[0]),activation='softmax')\n",
        "net=tflearn.regression(net)\n",
        "\n",
        "model=tflearn.DNN(net)\n",
        "try:\n",
        "  model.load(\"model.tflearn\")\n",
        "except:\n",
        "  model=tflearn.DNN(net)\n",
        "  model.fit(training,output,n_epoch=1000,batch_size=8,show_metric=True)\n",
        "  model.save(\"model.tflearn\")\n",
        "\n",
        "    \n",
        "  "
      ],
      "metadata": {
        "id": "UrxXNiXTN3N1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35001974-9576-41ee-8597-63aa67b5434f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 11999  | time: 0.079s\n",
            "| Adam | epoch: 1000 | loss: 0.00000 - acc: 0.9305 -- iter: 88/89\n",
            "Training Step: 12000  | time: 0.085s\n",
            "| Adam | epoch: 1000 | loss: 0.00000 - acc: 0.9374 -- iter: 89/89\n",
            "--\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(s,words):\n",
        "  bag=[0 for _ in range(len(words))]\n",
        "  s_words=nltk.word_tokenize(s)\n",
        "  s_words=[stemmer.stem(word.lower())for word in s_words]\n",
        "  for se in s_words:\n",
        "    for i,w in enumerate(words):\n",
        "      if w==se:\n",
        "        bag[i] = 1\n",
        "  return np.array(bag)"
      ],
      "metadata": {
        "id": "XIFC0stjJJRv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "  print(\"Hey I am Cera!\")\n",
        "  while True:\n",
        "    inp=input(\"You:\")\n",
        "    if inp.lower()==\"quit\" or inp.lower()==\"bye\":\n",
        "      break\n",
        "\n",
        "    results=model.predict([bag_of_words(inp,words)])\n",
        "    results_index=np.argmax(results)\n",
        "    tag=labels[results_index]\n",
        "\n",
        "    for tg in data[\"intents\"]:\n",
        "      if tg[\"tag\"]==tag:\n",
        "        response=tg['responses']\n",
        "\n",
        "    corpus_text = listToString(response)\n",
        "    print(\"Cera: \"+ (answer(corpus_text,inp)))"
      ],
      "metadata": {
        "id": "hmSAmUTKT1fu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def listToString(response):\n",
        "   \n",
        "    str1 = \" \"\n",
        "    return (str1.join(response))"
      ],
      "metadata": {
        "id": "xV9o1DnYQAz-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer(corpus_text,inp):\n",
        "  corpus_sentences=nltk.sent_tokenize(corpus_text)\n",
        "  corpus_words=nltk.word_tokenize(corpus_text)\n",
        "\n",
        "  wn_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "  def lemmatize_data(tokens):\n",
        "    return [wn_lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "  punct_remover=dict((ord(punctuation),None) for punctuation in string.punctuation)\n",
        "\n",
        "  def get_processed_data(data):\n",
        "    return lemmatize_data(nltk.word_tokenize(data.lower().translate(punct_remover)))\n",
        "\n",
        "  corpus_sentences.append(inp)\n",
        "\n",
        "  word_vectorizer=TfidfVectorizer(tokenizer=get_processed_data)\n",
        "  corpus_word_vectors=word_vectorizer.fit_transform(corpus_sentences)\n",
        "  cos_sin_vectors=cosine_similarity(corpus_word_vectors[-1],corpus_word_vectors)\n",
        "  similar_response_idx=cos_sin_vectors.argsort()[0][-2]\n",
        "\n",
        "  return corpus_sentences[similar_response_idx]\n"
      ],
      "metadata": {
        "id": "qTiLfvkQO_Xu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat()"
      ],
      "metadata": {
        "id": "3OnvmoHUg3XM",
        "outputId": "11c82b51-d2fd-4dfa-c44c-ef827e0f7984",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hey I am Cera!\n",
            "You:hey cera\n",
            "Cera: Hi there, how can I help?\n",
            "You:how can i cure pimples?\n",
            "Cera: You can use over-the-counter (OTC) medicated creams, cleansers, and spot treatments to help address mild acne and pimples as they pop up.\n",
            "You:bye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B5KIzojuhHNi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}